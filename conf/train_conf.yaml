# =========================
# Experiment configurations
# =========================
defaults:
  - loggers:
      - tensorboard
  - /callbacks:
      - lr_monitor
      - grad_norm
      - speed_monitor
      - model_checkpoint
  - model: llama
  - override /hydra/job_logging: colorlog
  - override /hydra/hydra_logging: colorlog
  - _self_

hydra:
  job_logging:
    formatters:
      colorlog:
        format: "[%(cyan)s%(asctime)s%(reset)s][%(blue)s%(name)s%(reset)s][%(log_color)s%(levelname)s%(reset)s] - %(message)s"
        log_colors:
          DEBUG: purple
          INFO: green
          WARNING: yellow
          ERROR: red
          CRITICAL: bold_red
  job:
    chdir: true
  run:
    dir: ./outputs/${out_parent_folder}/${run_folder}
  sweep:
    dir: ./outputs/multirun/${out_parent_folder}
    subdir: ${run_folder}_${hydra.job.id}

# Folder structure
pwd: ./
out_parent_folder: training
run_folder: run_${now:%Y-%m-%d}T${now:%H-%M-%S}

# Tokenizer
tok_path: "???"
tok_subfolder: null

# Data paths
train_data_path: "???"
val_data_path: "???"

# Model training
resume_from_checkpoint: null
save_initial_checkpoint: true

seed: 42
torch_compile: true
use_liger: true

data:
  batch_size: 32
  eval_batch_size: 64
  shuffle_seed: null  # no shuffling
  drop_last: true
  num_workers: 8
  pin_memory: true
  persistent_workers: false
  prefetch_factor: 2
  multiprocessing_context: null
  intra_doc_causal_mask: false

optim:
  optim_name: adamw
  lr: 6e-4
  grad_acc_schedule: { 0: 2 }
  weight_decay: 0.01
  optim_kwargs:
    fused: true
    eps: 1e-8
    betas: [0.9, 0.95]
  scheduler_name: warmup_stable_decay # cosine_with_min_lr
  num_warmup_steps: 2000
  scheduler_kwargs:
    num_decay_steps: 4000
    min_lr_ratio: 0.01

trainer:
  accelerator: gpu
  devices: 1
  precision: bf16-true
  deterministic: false
  log_every_n_steps: 1
  enable_progress_bar: true
  fast_dev_run: false
  gradient_clip_val: 1.0
  gradient_clip_algorithm: norm
  val_check_interval: 2000
  max_steps: 50_000
  limit_val_batches: null
  limit_train_batches: null
